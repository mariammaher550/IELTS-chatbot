{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## before and after finetuning for gpt-3"
   ],
   "metadata": {
    "id": "o9xxERHEPUKD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install BeautifulSoup4 --quiet"
   ],
   "metadata": {
    "id": "7vcJapEMKEea"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai --quiet"
   ],
   "metadata": {
    "id": "YsMd417URRSS"
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate --queit"
   ],
   "metadata": {
    "id": "2y3ZpcbgSP4A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install bert_score --quiet"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMhp922wSql2",
    "outputId": "70b174fe-87df-4e85-cdb8-6e5230bd4a25"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from google.colab import userdata\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from evaluate import load"
   ],
   "metadata": {
    "id": "pLGzrK_RJjmJ"
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "you are an IELTS examiner. your task is to evaluate a writing section in an IELTS academic\n",
    "exam. you have to provide overall band score in <BAND_SCORE> </BAND_SCORE> tags and detailed evaluation in <EVALUATION></EVALUATION> tags . I will provide you the grading\n",
    "criteria in <CRITERIA> </CRITERIA> tags. The user will send you the task and his answer and you should respond with a feedback on how well does the user follow the grading criteria and his score. Provide his score in this format <Score>Score</Score>.\n",
    "<CRITERIA>\n",
    "TASK RESPONSE (TR)\n",
    "For Task 2 of both AC and GT Writing tests, candidates are required to formulate and\n",
    "develop a position in relation to a given prompt in the form of a question or\n",
    "statement, using a minimum of 250 words. Ideas should be supported by evidence,\n",
    "and examples may be drawn from a candidate’s own experience.\n",
    "The TR criterion assesses:\n",
    "▪ how fully the candidate responds to the task.\n",
    "▪ how adequately the main ideas are extended and supported.\n",
    "▪ how relevant the candidate’s ideas are to the task.\n",
    "▪ how clearly the candidate opens the discourse, establishes their position and\n",
    "formulates conclusions.\n",
    "▪ how appropriate the format of the response is to the task.\n",
    "COHERENCE AND COHESION (CC)\n",
    "This criterion is concerned with the overall organisation and logical development of\n",
    "the message: how the response organises and links information, ideas and language.\n",
    "Coherence refers to the linking of ideas through logical sequencing, while cohesion\n",
    "refers to the varied and appropriate use of cohesive devices (e.g. logical connectors,\n",
    "conjunctions and pronouns) to assist in making clear the relationships between and\n",
    "within sentences.\n",
    "The CC criterion assesses:\n",
    "▪ the coherence of the response via the logical organisation of information\n",
    "and/or ideas, or the logical progression of the argument.\n",
    "▪ the appropriate use of paragraphing for topic organisation and presentation.\n",
    "▪ the logical sequencing of ideas and/or information within and across\n",
    "paragraphs.\n",
    "▪ the flexible use of reference and substitution (e.g. definite articles, pronouns).\n",
    "▪ the appropriate use of discourse markers to clearly mark the stages in a\n",
    "response, e.g. [First of all | In conclusion], and to signal the relationship between ideas and/or information, e.g. [as a result | similarly].\n",
    "\n",
    "LEXICAL RESOURCE (LR)\n",
    "This criterion refers to the range of vocabulary the candidate has used and the\n",
    "accuracy and appropriacy of that use in terms of the specific task.\n",
    "The LR criterion assesses:\n",
    "▪ the range of general words used (e.g. the use of synonyms to avoid repetition).\n",
    "▪ the adequacy and appropriacy of the vocabulary (e.g. topic-specific items,\n",
    "indicators of writer’s attitude).\n",
    "▪ the precision of word choice and expression.\n",
    "▪ the control and use of collocations, idiomatic expressions and sophisticated\n",
    "phrasing.\n",
    "▪ the density and communicative effect of errors in spelling.\n",
    "▪ the density and communicative effect of errors in word formation.\n",
    "GRAMMATICAL RANGE AND ACCURACY (GRA)\n",
    "This criterion refers to the range and accurate use of the candidate’s grammatical\n",
    "resource via the candidate’s writing at sentence level.\n",
    "The GRA criterion assesses:\n",
    "▪ the range and appropriacy of structures used in a given response (e.g. simple,\n",
    "compound and complex sentences).\n",
    "▪ the accuracy of simple, compound and complex sentences.\n",
    "▪ the density and communicative effect of grammatical errors.\n",
    "▪ the accurate and appropriate use of punctuation.\n",
    "</CRITERIA>\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "onwO-sOfgnFQ"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get responses before finetuning\n",
    "def get_responses(model_name, df, client, prompt):\n",
    "  responses = []\n",
    "  for index, row in df.iterrows():\n",
    "    message =  [{\"role\": \"system\", \"content\": prompt},\n",
    "              {\"role\": \"user\", \"content\": f\"Here is the task:\\n <Task>{row['Question']}</Task> \\n And here is my answer: \\n <Answer>{row['Answer']}</Answer>\"},\n",
    "            ]\n",
    "    response = client.chat.completions.create(\n",
    "      messages=message,\n",
    "      model = model_name,\n",
    "    )\n",
    "    responses.append(response)\n",
    "  return responses\n"
   ],
   "metadata": {
    "id": "Cg4QnOdp442A"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_squared_error(responses, y_true):\n",
    "  scores=[]\n",
    "  feedbacks=[]\n",
    "\n",
    "  for response in responses:\n",
    "    match = re.search(r'<score>(.*?)</score>', responses[0].choices[0].message.content.lower())\n",
    "    scores.append(match.group(1))\n",
    "\n",
    "  y_true = [float(x) for x in y_true]\n",
    "  y_pred = [float(x) for x in scores]\n",
    "\n",
    "  m = mean_squared_error(y_pred = y_pred, y_true=y_true)\n",
    "  return m"
   ],
   "metadata": {
    "id": "T9tK_5DJPyZE"
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_semantic_similarity(responses, feedbacks, scores):\n",
    "  bertscore = load(\"bertscore\")\n",
    "  predictions = [response.choices[0].message.content for response in responses]\n",
    "  references = [f\"<Score>{score}</Score> \\n feedback : {feedback}\" for score, feedback in zip(scores, feedbacks)]\n",
    "\n",
    "  results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "  return sum(results['f1']) / len(results['f1'])\n"
   ],
   "metadata": {
    "id": "zAe_FecQRiOm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# BEFORE FINETUNING\n",
    "api_key = userdata.get(\"openai_api_key\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "df = pd.read_csv('ielts_buddy_test_dataset.csv')\n",
    "responses = get_responses(model_name, df, client, prompt)\n",
    "mse = get_squared_error(responses, df[\"Final Score\"].tolist())\n",
    "semantic_similarity = get_semantic_similarity(responses, df[\"Feedback\"], df[\"Final Score\"])\n",
    "print(\"mean_squared_error after finetuning: \", mse)\n",
    "print(\"semantic_similarity: after finetuning\", semantic_similarity)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N49_63BQU7HA",
    "outputId": "55f7df0a-4435-4a98-a90c-3b7f90b96b63"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean_squared_error after finetuning:  0.5\n",
      "semantic_similarity: after finetuning 0.8347912430763245\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# AFTER FINETUNING\n",
    "api_key = userdata.get(\"openai_api_key\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "model_name = userdata.get(\"finetuned_model\")\n",
    "df = pd.read_csv('ielts_buddy_test_dataset.csv')\n",
    "#responses = get_responses(model_name, df, client, prompt)\n",
    "mse = get_squared_error(responses, df[\"Final Score\"].tolist())\n",
    "semantic_similarity = get_semantic_similarity(responses, df[\"Feedback\"], df[\"Final Score\"])\n",
    "print(\"mean_squared_error after finetuning: \", mse)\n",
    "print(\"semantic_similarity: after finetuning\", semantic_similarity)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJmvdAYWG_ZB",
    "outputId": "a59f6e2f-7d8f-4aea-af61-d475c2eb7813"
   },
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean_squared_error:  0.35\n",
      "semantic_similarity:  0.845073914527893\n"
     ]
    }
   ]
  }
 ]
}
