{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sH_r5qPMOqcI",
        "outputId": "658f0889-30ae-4767-c207-7740ca5d996e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<4,>=3.5.0 (from openai)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distro<2,>=1.7.0 (from openai)\n",
            "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>4 (from openai)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.5 (from openai)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting idna>=2.8 (from anyio<4,>=3.5.0->openai)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.1 (from anyio<4,>=3.5.0->openai)\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup (from anyio<4,>=3.5.0->openai)\n",
            "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, tqdm, sniffio, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.3\n",
            "    Uninstalling exceptiongroup-1.1.3:\n",
            "      Successfully uninstalled exceptiongroup-1.1.3\n",
            "  Attempting uninstall: distro\n",
            "    Found existing installation: distro 1.7.0\n",
            "    Uninstalling distro-1.7.0:\n",
            "      Successfully uninstalled distro-1.7.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.7.22\n",
            "    Uninstalling certifi-2023.7.22:\n",
            "      Successfully uninstalled certifi-2023.7.22\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-3.7.1 certifi-2023.11.17 distro-1.8.0 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 idna-3.4 openai-1.3.5 pydantic-2.5.2 pydantic-core-2.14.5 sniffio-1.3.0 tqdm-4.66.1 typing-extensions-4.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install openai --force-reinstall\n",
        "!pip install evaluate\n",
        "!pip install bert_score --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9xxERHEPUKD"
      },
      "source": [
        "## before and after finetuning davinci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "pLGzrK_RJjmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "onwO-sOfgnFQ"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "you are an IELTS examiner. your task is to evaluate a writing section in an IELTS academic\n",
        "exam. you have to provide overall band score in <BAND_SCORE> </BAND_SCORE> tags and detailed evaluation in <EVALUATION></EVALUATION> tags . I will provide you the grading\n",
        "criteria in <CRITERIA> </CRITERIA> tags. The user will send you the task and his answer and you should respond with a feedback on how well does the user follow the grading criteria and his score. Provide his score in this format <Score>Score</Score>.\n",
        "<CRITERIA>\n",
        "TASK RESPONSE (TR)\n",
        "For Task 2 of both AC and GT Writing tests, candidates are required to formulate and\n",
        "develop a position in relation to a given prompt in the form of a question or\n",
        "statement, using a minimum of 250 words. Ideas should be supported by evidence,\n",
        "and examples may be drawn from a candidate’s own experience.\n",
        "The TR criterion assesses:\n",
        "▪ how fully the candidate responds to the task.\n",
        "▪ how adequately the main ideas are extended and supported.\n",
        "▪ how relevant the candidate’s ideas are to the task.\n",
        "▪ how clearly the candidate opens the discourse, establishes their position and\n",
        "formulates conclusions.\n",
        "▪ how appropriate the format of the response is to the task.\n",
        "COHERENCE AND COHESION (CC)\n",
        "This criterion is concerned with the overall organisation and logical development of\n",
        "the message: how the response organises and links information, ideas and language.\n",
        "Coherence refers to the linking of ideas through logical sequencing, while cohesion\n",
        "refers to the varied and appropriate use of cohesive devices (e.g. logical connectors,\n",
        "conjunctions and pronouns) to assist in making clear the relationships between and\n",
        "within sentences.\n",
        "The CC criterion assesses:\n",
        "▪ the coherence of the response via the logical organisation of information\n",
        "and/or ideas, or the logical progression of the argument.\n",
        "▪ the appropriate use of paragraphing for topic organisation and presentation.\n",
        "▪ the logical sequencing of ideas and/or information within and across\n",
        "paragraphs.\n",
        "▪ the flexible use of reference and substitution (e.g. definite articles, pronouns).\n",
        "▪ the appropriate use of discourse markers to clearly mark the stages in a\n",
        "response, e.g. [First of all | In conclusion], and to signal the relationship between ideas and/or information, e.g. [as a result | similarly].\n",
        "\n",
        "LEXICAL RESOURCE (LR)\n",
        "This criterion refers to the range of vocabulary the candidate has used and the\n",
        "accuracy and appropriacy of that use in terms of the specific task.\n",
        "The LR criterion assesses:\n",
        "▪ the range of general words used (e.g. the use of synonyms to avoid repetition).\n",
        "▪ the adequacy and appropriacy of the vocabulary (e.g. topic-specific items,\n",
        "indicators of writer’s attitude).\n",
        "▪ the precision of word choice and expression.\n",
        "▪ the control and use of collocations, idiomatic expressions and sophisticated\n",
        "phrasing.\n",
        "▪ the density and communicative effect of errors in spelling.\n",
        "▪ the density and communicative effect of errors in word formation.\n",
        "GRAMMATICAL RANGE AND ACCURACY (GRA)\n",
        "This criterion refers to the range and accurate use of the candidate’s grammatical\n",
        "resource via the candidate’s writing at sentence level.\n",
        "The GRA criterion assesses:\n",
        "▪ the range and appropriacy of structures used in a given response (e.g. simple,\n",
        "compound and complex sentences).\n",
        "▪ the accuracy of simple, compound and complex sentences.\n",
        "▪ the density and communicative effect of grammatical errors.\n",
        "▪ the accurate and appropriate use of punctuation.\n",
        "</CRITERIA>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Cg4QnOdp442A"
      },
      "outputs": [],
      "source": [
        "# get responses before finetuning\n",
        "def get_responses(model_name, df, client, prompt):\n",
        "  responses = []\n",
        "  for index, row in df.iterrows():\n",
        "    message = prompt + f\"Here is the task:\\n <Task>{row['Question']}</Task> \\n And here is my answer: \\n <Answer>{row['Answer']}</Answer>\\n Feedback: \"\n",
        "\n",
        "    response = client.completions.create(\n",
        "                model=model_name,\n",
        "                prompt=message,\n",
        "                max_tokens=1024,\n",
        "                top_p=1\n",
        "            )\n",
        "    responses.append(response)\n",
        "  return responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "T9tK_5DJPyZE"
      },
      "outputs": [],
      "source": [
        "def extract_score(text):\n",
        "    \"\"\"\n",
        "      Extracting the score from the model's answer\n",
        "    \"\"\"\n",
        "    numbers = re.findall(r'\\d+\\.\\d+|\\d+', text)\n",
        "    return float(numbers[-1]) if numbers and float(numbers[-1]) <=9 else 6.5\n",
        "\n",
        "\n",
        "def get_squared_error(responses, y_true):\n",
        "  scores=[]\n",
        "  feedbacks=[]\n",
        "\n",
        "  for response in responses:\n",
        "    score = extract_score(response.choices[0].text.lower())\n",
        "    scores.append(score)\n",
        "\n",
        "  y_true = [float(x) for x in y_true]\n",
        "  y_pred = [float(x) for x in scores]\n",
        "\n",
        "  m = mean_squared_error(y_pred = y_pred, y_true=y_true)\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "zAe_FecQRiOm"
      },
      "outputs": [],
      "source": [
        "def get_semantic_similarity(responses, feedbacks, scores):\n",
        "  bertscore = load(\"bertscore\")\n",
        "  predictions = [response.choices[0].text for response in responses]\n",
        "  references = [f\"<Score>{score}</Score> \\n feedback : {feedback}\" for score, feedback in zip(scores, feedbacks)]\n",
        "\n",
        "  results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "  return sum(results['f1']) / len(results['f1'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "N49_63BQU7HA"
      },
      "outputs": [],
      "source": [
        "# BEFORE FINETUNING\n",
        "api_key = r\"\"\n",
        "client = OpenAI(api_key=api_key)\n",
        "model_name = \"davinci-002\"\n",
        "df = pd.read_csv('ielts_buddy_test_dataset.csv')\n",
        "responses = get_responses(model_name, df, client, prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse = get_squared_error(responses, df[\"Final Score\"].tolist())\n",
        "semantic_similarity = get_semantic_similarity(responses, df[\"Feedback\"], df[\"Final Score\"])\n",
        "print(\"mean_squared_error after finetuning: \", mse)\n",
        "print(\"semantic_similarity: after finetuning\", semantic_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Pe92WjSJ0C",
        "outputId": "c0417921-bb62-494b-8434-96c344faf7b0"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_squared_error after finetuning:  10.7\n",
            "semantic_similarity: after finetuning 0.8055064797401428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJmvdAYWG_ZB",
        "outputId": "874aba7e-1ed0-4f49-ada7-dda9c9e93da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_squared_error after finetuning:  0.35\n",
            "semantic_similarity: after finetuning 0.8257498621940613\n"
          ]
        }
      ],
      "source": [
        "# AFTER FINETUNING\n",
        "api_key = \"\"\n",
        "client = OpenAI(api_key=api_key)\n",
        "model_name = \"\"\n",
        "df = pd.read_csv('ielts_buddy_test_dataset.csv')\n",
        "responses = get_responses(model_name, df, client, prompt)\n",
        "mse = get_squared_error(responses, df[\"Final Score\"].tolist())\n",
        "semantic_similarity = get_semantic_similarity(responses, df[\"Feedback\"], df[\"Final Score\"])\n",
        "print(\"mean_squared_error after finetuning: \", mse)\n",
        "print(\"semantic_similarity: after finetuning\", semantic_similarity)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}